{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32a02ae5-f175-46e9-952c-707bd11a4211",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Import statements\n",
    "import re\n",
    "import json\n",
    "import random\n",
    "import hidateinfer\n",
    "import numpy as np\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.functions import col, lower, udf, count, when, round, size, lit, row_number, length, translate, expr, sum as sum_, split, explode, collect_list, desc, broadcast\n",
    "from pyspark.sql.types import StringType, BooleanType, DateType, DecimalType, DoubleType, ArrayType\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import DataFrame\n",
    "from collections import defaultdict, Counter\n",
    "from operator import add\n",
    "from functools import reduce\n",
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "754ba1e6-74ae-4ef8-b456-dfed5aea7f4e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils/util_funcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4c2c2cc5-1683-46fd-a9f8-ef76ae02d98c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import decimal\n",
    "def myconverter(obj):\n",
    "    if isinstance(obj, np.integer):\n",
    "        return int(obj)\n",
    "    elif isinstance(obj, np.floating):\n",
    "        return float(obj)\n",
    "    elif isinstance(obj, np.ndarray):\n",
    "        return obj.tolist()\n",
    "    elif isinstance(obj, np.bool_):\n",
    "        return bool(obj)\n",
    "    elif isinstance(obj, (datetime.datetime, datetime.date)):\n",
    "        return obj.isoformat()\n",
    "    elif isinstance(obj, decimal.Decimal):\n",
    "        return str(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a2feeac-192b-4ad4-998a-85ea1a8dde50",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandasql as psql\n",
    "\n",
    "# import seaborn as sns\n",
    "# sns.set_style('whitegrid')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option('display.max_columns' , None)\n",
    "pd.set_option('display.max_rows' , None)\n",
    "pd.set_option('display.max_colwidth' , None)\n",
    "pd.set_option('max_info_columns' , -1)\n",
    "pd.set_option('max_info_rows' , -1)\n",
    "\n",
    "# import warnings\n",
    "# from pandas.core.common import SettingWithCopyWarning\n",
    "# warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"py4j\").setLevel(logging.ERROR)\n",
    "from collections import defaultdict,Counter\n",
    "import random\n",
    "\n",
    "#added by sahil\n",
    "import string\n",
    "import os\n",
    "from dataprep.clean import *\n",
    "import zipcodes\n",
    "from uszipcode import SearchEngine\n",
    "engine = SearchEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dacf420e-46f8-4417-8300-a863ba8c2673",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "\n",
    "dbutils.widgets.text(\"container_name\",\"none\",label=\"container_name\")\n",
    "dbutils.widgets.text(\"AZURE_STORAGE_CONNECTION_STRING\",\"none\",label=\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "dbutils.widgets.text(\"filename\",\"none\",label=\"filename\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4da513fd-8830-4773-8073-ac90a3bf7e3e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from azure.storage.blob import BlobServiceClient, BlobClient, ContainerClient\n",
    "import json\n",
    "import io\n",
    "\n",
    "# Get the values from the widgets\n",
    "filename = dbutils.widgets.get(\"filename\")\n",
    "que = dbutils.widgets.get(\"que\")\n",
    "\n",
    "destination_storage_type = dbutils.widgets.get(\"destination_storage_type\")\n",
    "if destination_storage_type == 'ABS':\n",
    "    CONTAINER_NAME = dbutils.widgets.get(\"container_name\")\n",
    "    AZURE_STORAGE_CONNECTION_STRING = dbutils.widgets.get(\"AZURE_STORAGE_CONNECTION_STRING\")\n",
    "elif destination_storage_type == 'S3':\n",
    "    storage_access_key = dbutils.widgets.get(\"storage_access_key\")\n",
    "    storage_secret_key = dbutils.widgets.get(\"storage_secret_key\")\n",
    "    bucket_name = dbutils.widgets.get(\"bucket_name\")\n",
    "else:\n",
    "    print(f\"Invalid destination_storage_type {destination_storage_type}\")\n",
    "\n",
    "if destination_storage_type == 'ABS':\n",
    "    # Create a BlobServiceClient object using the connection string\n",
    "    BLOB_SERVICE_CLIENT = BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)\n",
    "\n",
    "\n",
    "filename_zeroshot1 = filename + '.zeroshot'\n",
    "filename_zeroshot2 = filename + '.zeroshot2'\n",
    "print(filename)\n",
    "print(filename_zeroshot1)\n",
    "print(filename_zeroshot2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ff9e090a-d6a0-49a3-b1ae-e9f686a5b6c9",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def infer_datatypes(df, column_name):\n",
    "    \"\"\"\n",
    "    Infer the data types of a specified column in a PySpark DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (DataFrame): The PySpark DataFrame to analyze.\n",
    "        column_name (str): The name of the column to analyze.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary containing the counts of StringType, DateType, and Numeric values,\n",
    "              along with the majority data type.\n",
    "\n",
    "    \"\"\"\n",
    "    str_count = 0\n",
    "    date_count = 0\n",
    "    num_count = 0\n",
    "\n",
    "    if column_name in df.columns:\n",
    "        column_type = df.schema[column_name].dataType\n",
    "        if column_type == StringType():\n",
    "            str_count = df.filter(F.col(column_name).isNotNull()).count()\n",
    "        elif column_type == DateType():\n",
    "            date_count = df.filter(F.col(column_name).isNotNull()).count()\n",
    "        else:\n",
    "            num_count = df.filter(F.col(column_name).isNotNull()).count()\n",
    "\n",
    "    data_types_dict = {\n",
    "        \"Non-Numeric\": str_count,\n",
    "        \"Date\": date_count,\n",
    "        \"Numeric\": num_count,\n",
    "        \"Majority Type\": get_majority_type(str_count, date_count, num_count)\n",
    "    }\n",
    "\n",
    "    return data_types_dict\n",
    "\n",
    "def get_majority_type(str_count, date_count, num_count):\n",
    "    \"\"\"\n",
    "    Determine the majority data type based on the counts of StringType, DateType, and Numeric values.\n",
    "\n",
    "    Args:\n",
    "        str_count (int): The count of StringType values.\n",
    "        date_count (int): The count of DateType values.\n",
    "        num_count (int): The count of Numeric values.\n",
    "\n",
    "    Returns:\n",
    "        str: The majority data type (\"String\", \"Date\", \"Numeric\", or \"Unknown\" if there's a tie).\n",
    "\n",
    "    \"\"\"\n",
    "    if str_count > date_count and str_count > num_count:\n",
    "        return \"Non-Numeric\"\n",
    "    elif date_count > str_count and date_count > num_count:\n",
    "        return \"Date\"\n",
    "    elif num_count > str_count and num_count > date_count:\n",
    "        return \"Numeric\"\n",
    "    else:\n",
    "        return \"Unknown\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "736bce5c-bdd4-499e-8124-7ce67a06a081",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_only_special_chars(text):\n",
    "    if all(not i.isalnum() for i in text):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "def filter_blank(x):\n",
    "    if ''.join(set(str(x))) in [\"\", \" \"]:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "class GlobalFuncs():\n",
    "    global get_num_cols\n",
    "    def get_num_cols(df):\n",
    "        new = df.copy()\n",
    "        num_cols = []\n",
    "        new = new.astype(str)\n",
    "        for i in new.columns:\n",
    "            new[i] = new[i].apply(lambda x : replace_only_special_chars(x)).astype(str)\n",
    "            try:\n",
    "                if new[i].astype(float).isnull().sum() != len(new):\n",
    "                    new[i] = new[i].apply(lambda x : float(x.strip().replace(\",\",\"\")))\n",
    "                    n = int(len(new[i].dropna())/5)\n",
    "                    if abs(new[i].sample(n).mean()) >=0:\n",
    "                        #print(i, df[i].sample(n).mean())\n",
    "                        num_cols.append(i)\n",
    "            except:\n",
    "                pass\n",
    "        return new, num_cols\n",
    "\n",
    "    \n",
    "    global type_casting\n",
    "    def type_casting(df, dtypes, num=True, non_num=True, date=True):\n",
    "        df = df.reset_index(drop=True)\n",
    "        #df = df.astype(str)\n",
    "        if num==True:\n",
    "            for i in dtypes[\"Numeric\"]:\n",
    "                df[i] = df[i].astype(str).apply(lambda x : float(x.strip().replace(\",\",\"\")))\n",
    "        if date==True:\n",
    "            for i in dtypes[\"Date\"]:\n",
    "                df[i] = df[i].astype(str).apply(lambda x : x.strip())\n",
    "                df.loc[df[i].isin([\"nan\", \"NAN\", \"NaN\", \"Nan\", \"NaT\"]), i] = np.nan\n",
    "                df[i] = pd.to_datetime(df[i], errors ='coerce')\n",
    "        if non_num ==True:\n",
    "            for i in dtypes[\"Non-Numeric\"]:\n",
    "                df[i] = df[i].astype(str).apply(lambda x : x.strip())\n",
    "                df.loc[df[i].isin([\"nan\", \"NAN\", \"NaN\", \"Nan\"]), i] = np.nan\n",
    "        return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9c28aba1-36e8-45dd-a641-acdf7fd6f592",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_special_char(word):\n",
    "    final = re.sub(r\"[^a-zA-Z0-9.]+\", ' ', word)\n",
    "    final = re.sub(\"\\s\\s+\", \" \", final)\n",
    "    return final.strip()\n",
    "    \n",
    "def check_special_char(word):\n",
    "    #word = re.sub(r'([^\\s])\\s([^\\s])', r'\\1_\\2', word)\n",
    "    #whitespaces = re.findall(r\"\\s+\", word)\n",
    "    sp_chars = [c for c in word if not c.isalnum()]\n",
    "    sp_chars = [y for y in sp_chars if y != \" \"]\n",
    "    #sp_chars = sp_chars + spaces\n",
    "    #unique_sp_chars = list(set(sp_chars))\n",
    "    return sp_chars\n",
    "\n",
    "def check_whitespaces(word):\n",
    "    whitespaces = re.findall(r\"\\s+\", word)\n",
    "    return whitespaces\n",
    "  \n",
    "def check_non_ascii(word):\n",
    "    non_ascii_chars = [i for i in word if not i.isascii()]\n",
    "    return non_ascii_chars\n",
    "  \n",
    "def check_ascii(word):\n",
    "    #word = word.replace(\" \", \"\")\n",
    "    ascii_chars = [c for c in word if c.isascii() and not c.isspace()]\n",
    "    return ascii_chars\n",
    "    \n",
    "def check_alnum(word):\n",
    "    alnum_chars = [c for c in word if c.isalnum() and c.isascii()]\n",
    "    return alnum_chars\n",
    "    \n",
    "def check_all_char(word):\n",
    "    \n",
    "    len_sp_chars = len(check_special_char(word))\n",
    "    len_whitespaces = len(check_whitespaces(word))\n",
    "    len_non_ascii_chars = len(check_non_ascii(word))\n",
    "    len_alnum_chars = len(check_alnum(word))\n",
    "    \n",
    "    return len_sp_chars + len_whitespaces + len_non_ascii_chars + len_alnum_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "28c96ba6-3b74-46ac-9078-f5d7bec93766",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def mode(ndarray, axis=0):\n",
    "    # Check inputs\n",
    "    #ndarray = np.asarray(ndarray)\n",
    "    ndim = ndarray.ndim\n",
    "    if ndarray.size == 1:\n",
    "        return (ndarray[0], 1)\n",
    "    elif ndarray.size == 0:\n",
    "        #raise Exception('Cannot compute mode on empty array')\n",
    "        return ([], [])\n",
    "    try:\n",
    "        axis = range(ndarray.ndim)[axis]\n",
    "    except:\n",
    "        raise Exception('Axis \"{}\" incompatible with the {}-dimension array'.format(axis, ndim))\n",
    "\n",
    "    # If array is 1-D and np version is > 1.9 np.unique will suffice\n",
    "    #if all([ndim == 1,\n",
    "            #int(np.__version__.split('.')[0]) >= 1,\n",
    "            #int(np.__version__.split('.')[1]) >= 9]):\n",
    "    if ndim ==1:\n",
    "        modals, counts = np.unique(ndarray, return_counts=True)\n",
    "        index = np.argmax(counts) #counts.argmax()\n",
    "        return modals[index], counts[index]\n",
    "\n",
    "    # Sort array\n",
    "    sort = np.sort(ndarray, axis=axis)\n",
    "    # Create array to transpose along the axis and get padding shape\n",
    "    transpose = np.roll(np.arange(ndim)[::-1], axis)\n",
    "    shape = list(sort.shape)\n",
    "    shape[axis] = 1\n",
    "    # Create a boolean array along strides of unique values\n",
    "    strides = np.concatenate([np.zeros(shape=shape, dtype='bool'),\n",
    "                                 np.diff(sort, axis=axis) == 0,\n",
    "                                 np.zeros(shape=shape, dtype='bool')],\n",
    "                                axis=axis).transpose(transpose).ravel()\n",
    "    # Count the stride lengths\n",
    "    counts = np.cumsum(strides)\n",
    "    counts[~strides] = np.concatenate([[0], np.diff(counts[~strides])])\n",
    "    counts[strides] = 0\n",
    "    # Get shape of padded counts and slice to return to the original shape\n",
    "    shape = np.array(sort.shape)\n",
    "    shape[axis] += 1\n",
    "    shape = shape[transpose]\n",
    "    slices = [slice(None)] * ndim\n",
    "    slices[axis] = slice(1, None)\n",
    "    # Reshape and compute final counts\n",
    "    counts = counts.reshape(shape).transpose(transpose)[slices] + 1\n",
    "\n",
    "    # Find maximum counts and return modals/counts\n",
    "    slices = [slice(None, i) for i in sort.shape]\n",
    "    del slices[axis]\n",
    "    index = np.ogrid[slices]\n",
    "    index.insert(axis, np.argmax(counts, axis=axis))\n",
    "    return sort[index], counts[index]\n",
    "\n",
    "from scipy import stats as st\n",
    "import pylab as pl\n",
    "\n",
    "class AllStats():\n",
    "    def __init__(self,yourarray):\n",
    "        v=yourarray\n",
    "        self.mini= float(min(v))\n",
    "        self.q1 = float(np.percentile(v, 25))\n",
    "        self.q2 = float(np.percentile(v, 50))\n",
    "        self.q3 = float(np.percentile(v, 75))\n",
    "        self.maxi= float(max(v))\n",
    "        self.mean= float(np.mean(v))\n",
    "        self.median= float(np.median(v))\n",
    "        self.mode= st.mode(v)[0][0]\n",
    "        self.count= float(v.shape[0])\n",
    "        self.range= float(max(v)-min(v))\n",
    "        self.iqr = float( np.percentile(v, 75)-np.percentile(v, 25) )\n",
    "        self.std= float(np.std(v))\n",
    "        self.mad = float(st.median_abs_deviation(v))\n",
    "        self.var= float(np.var(v))\n",
    "        self.coeffvar= float(((np.std(v))/(np.mean(v)))*100)\n",
    "        self.skewness = float(st.skew(v))\n",
    "        self.kurtosis = float(st.kurtosis(v))\n",
    "        \n",
    "    def summarystats(self, v):\n",
    "        mylist= [self.mini, self.q1, self.q2, self.q3, self.maxi, self.mean, self.median, self.mode, self.count,\n",
    "                 self.range, self.iqr, self.std, self.mad, self.var, self.coeffvar, self.skewness, self.kurtosis]\n",
    "        d= {'Summary Statistics': mylist}\n",
    "        listnames = ['Min', 'First Quartile: Q1 (25%)', 'Second Quartile: Q2 (50%)', 'Third Quartile: Q3 (75%)', 'Max',\n",
    "                    'Mean', 'Median', 'Mode', 'Count', 'Range', 'IQR', 'StdDev', 'MAD', 'Variance', 'Coeff. of Variation',\n",
    "                    'Skewness', 'Kurtosis']\n",
    "        return ( pd.DataFrame(data= d, index= listnames) )  \n",
    "    \n",
    "    def outliers_zscore(self,v):\n",
    "        threshold =2 #3.0\n",
    "        self.zscores = [(i- self.mean)/self.std for i in v]\n",
    "        self.outliers = [v[i] for i in range(len(self.zscores)) if np.abs(self.zscores[i]) > threshold]\n",
    "        return self\n",
    "    \n",
    "    def outliers_modified_zscore(self,v):\n",
    "        #ST DEV ≈ 1.155 MAD (uniform dist)\n",
    "        #ST DEV ≈ 1.254 MAD for small samples (MAD <2.5) and 1.4826 MAD for large samples (MAD>=2.5) (normal dist)\n",
    "        threshold = 2.508 #3.762\n",
    "        median_y = self.median\n",
    "        median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in v])\n",
    "        self.modified_z_scores = [0.7974 * (y - median_y) / median_absolute_deviation_y for y in v] #0.6745\n",
    "        self.outliers = [v[i] for i in range(len(self.modified_z_scores)) if np.abs(self.modified_z_scores[i]) > threshold]\n",
    "        return self\n",
    "    \n",
    "    def outliers_iqr(self,v):\n",
    "        quartile_1, quartile_3 = np.percentile(v, [25, 75])\n",
    "        iqr = quartile_3 - quartile_1\n",
    "        lower_bound = quartile_1 - (iqr * 1.5)\n",
    "        upper_bound = quartile_3 + (iqr * 1.5)\n",
    "        self.outliers= np.where((v > upper_bound) | (v < lower_bound))\n",
    "        return self\n",
    "\n",
    "    def outliers_std(self,v):\n",
    "        #ST DEV ≈ 1.155 MAD (uniform dist)\n",
    "        #ST DEV ≈ 1.254 MAD for small samples (MAD <2.5) and 1.4826 MAD for large samples (MAD>=2.5) (normal dist)\n",
    "        self.outliersmean=[]\n",
    "        self.outliersmedian=[]\n",
    "        median_y = self.median\n",
    "        median_absolute_deviation_y = np.median([np.abs(y - median_y) for y in v])\n",
    "        for x in v:\n",
    "            if ( (x < (self.mean - (2 * self.std))) or (x > (self.mean + (2 * self.std))) ):\n",
    "                self.outliersmean.append(x)\n",
    "            if ( (x < (self.median - (2.508 * median_absolute_deviation_y))) or (x > (self.median + (2.508 * median_absolute_deviation_y))) ):\n",
    "            #if ( (x < (self.median - (2 * self.std))) or (x > (self.median + (2 * self.std))) ):\n",
    "                self.outliersmedian.append(x)\n",
    "            else:\n",
    "                continue\n",
    "        return self\n",
    "    \n",
    "        \n",
    "    def distribution_mean(self, v):\n",
    "        #v = sorted(v)\n",
    "        fit = st.norm.pdf(v, self.mean, self.std)\n",
    "        pl.plot(v,fit,'--')\n",
    "        pl.hist(v,density=True)\n",
    "        plt.axvline( (self.mean - 2*(self.std) ), color= 'g')\n",
    "        plt.axvline( (self.mean + 2*(self.std) ), color= 'g')\n",
    "        pl.show()\n",
    "        \n",
    "\n",
    "    def distribution_med(self, v):\n",
    "        #v = sorted(v)\n",
    "        fit = st.norm.pdf(v,self.median, self.std)\n",
    "        pl.plot(v,fit,'--')\n",
    "        pl.hist(v,density=True)\n",
    "        plt.axvline( (self.median - (2.508 * median_absolute_deviation_y)), color= 'g')\n",
    "        plt.axvline( (self.median + (2.508 * median_absolute_deviation_y)), color ='g')\n",
    "        pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6e4fce38-deb6-4ebd-9632-3e3a0e95841d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "que = dbutils.widgets.get(\"que\")\n",
    "sdf = sqlContext.sql(que)\n",
    "df = sdf.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da9013e1-efdd-494e-98d6-289f384ee2fc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rows_count = sdf.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "22c6a961-3a66-48d9-ac59-13db308cf24f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = df.fillna(np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2cd2f9ef-b09e-4688-8982-a96327752c14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def replace_only_special_chars(text):\n",
    "    if all(not i.isalnum() for i in text):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a19e5d3-48ee-4c39-baf1-be7ec35df1f1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def zeroshot_initial_analysis(df):\n",
    "\n",
    "# '''This function calculates statistics such as the count of blank, NaN, and null values, identifies the data type and inferred data type, counts distinct and duplicate values, finds the most frequent value, and calculates length statistics. The function collects these results into a list of dictionaries, where each dictionary represents the analysis results for a specific column.'''\n",
    "\n",
    "    # Initialize the list that will hold our analysis results\n",
    "    list_of_analysis = []\n",
    "\n",
    "    # Get the list of column names from the DataFrame\n",
    "    columnNames = df.columns\n",
    "\n",
    "    # Iterate over each column in the DataFrame\n",
    "    for columnName in columnNames:\n",
    "        # Initialize an empty dictionary to hold the analysis results for this column\n",
    "        att_dict = {}\n",
    "\n",
    "        # Set the attribute name in the dictionary\n",
    "        att_dict[\"attributeName\"] = columnName\n",
    "\n",
    "        # Count the number of blank strings in the column and store it in the dictionary\n",
    "        att_dict[\"blankCount\"] = df.filter((F.col(columnName) == ' ') | (F.col(columnName) == '')).count()\n",
    "\n",
    "        # If the column is of DoubleType, count the number of NaN values, otherwise count the number of null values\n",
    "        if df.schema[columnName].dataType == DoubleType():\n",
    "            att_dict[\"nanCount\"] = df.filter(F.isnan(columnName)).count()\n",
    "        else:\n",
    "            att_dict[\"nanCount\"] = df.filter(F.col(columnName).isNull()).count()\n",
    "\n",
    "        # Count the number of null values in the column and store it in the dictionary\n",
    "        att_dict[\"nullCount\"] = df.filter(F.col(columnName).isNull() | F.col(columnName).isin([\"Null\", \"null\", \"NULL\", \"NaT\"])).count()\n",
    "\n",
    "        # Store the data type of the column\n",
    "        att_dict[\"attributeDataType\"] = df.schema[columnName].dataType.simpleString().capitalize()\n",
    "\n",
    "        # Store the inferred data type of the column\n",
    "        att_dict[\"inferredDataType\"] = infer_datatypes(df,columnName)['Majority Type']\n",
    "\n",
    "        # Count the number of distinct values in the column and store it in the dictionary\n",
    "        att_dict[\"distinctCount\"] = df.select(columnName).distinct().count()\n",
    "\n",
    "        # Create a DataFrame that contains each value in the column and the count of that value\n",
    "        duplicate_df = df.groupBy(columnName).count()\n",
    "\n",
    "        # Count the number of duplicates in the column and store it in the dictionary\n",
    "        att_dict[\"duplicateCount\"] = duplicate_df.filter(F.col('count') > 1).count()\n",
    "\n",
    "        # Count the number of unique values in the column and store it in the dictionary\n",
    "        att_dict[\"uniqueCount\"] = duplicate_df.filter(F.col('count') == 1).count()\n",
    "\n",
    "        # Find the most frequent value in the column and its count and store them in the dictionary\n",
    "        most_freq_value = df.groupBy(columnName).count().orderBy(F.desc('count')).first()\n",
    "        att_dict[\"mostFrequentValue\"] = [str(most_freq_value[0])]\n",
    "        att_dict[\"mostFrequentCount\"] = [most_freq_value[1]]\n",
    "\n",
    "        # Calculate the minimum, maximum, mean, and standard deviation of the length of the values in the column\n",
    "        # Exclude null values from this calculation\n",
    "        # Cache the result to improve performance\n",
    "        lengthAnalysisDF = df.filter(F.col(columnName).isNotNull()).select(\n",
    "            F.min(F.length(F.col(columnName))).alias('minLength'),\n",
    "            F.max(F.length(F.col(columnName))).alias('maxLength'),\n",
    "            F.avg(F.length(F.col(columnName))).alias('meanLength'),\n",
    "            F.stddev(F.length(F.col(columnName))).alias('stddevLength')\n",
    "        ).cache()\n",
    "\n",
    "        firstRow = lengthAnalysisDF.first()\n",
    "        att_dict[\"minLength\"] = firstRow['minLength']\n",
    "        att_dict[\"maxLength\"] = firstRow['maxLength']\n",
    "        att_dict[\"meanLength\"] = firstRow['meanLength']\n",
    "        att_dict[\"standaradDeviationLength\"] = firstRow['stddevLength']\n",
    "\n",
    "        lengthAnalysisDF.unpersist()\n",
    "\n",
    "        list_of_analysis.append(att_dict)\n",
    "    return list_of_analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8d366755-3424-4821-b6db-fa8f10ddde81",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "list_of_analysis = zeroshot_initial_analysis(sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f541501-5409-4cf7-bfa4-dfd3faf74f0b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "json_filez1 = {\"attributeAnalysis\": list_of_analysis,\n",
    "                \"entityCount\": len(df)} #\"outputLocation\": filename_zeroshot1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e537d4a-9bd7-43a2-ba6a-0e265c0f3491",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store outputs in differnet storage based on destination_storage_type\n",
    "\n",
    "if destination_storage_type == 'ABS':\n",
    "\n",
    "    blob_clientz1 = BLOB_SERVICE_CLIENT.get_blob_client(\n",
    "    container=CONTAINER_NAME, blob=filename_zeroshot1)\n",
    "\n",
    "    blob_clientz1.upload_blob(json.dumps(json_filez1, indent=2, default=myconverter).encode('utf-8'), overwrite=True)\n",
    "elif destination_storage_type == 'S3':\n",
    "    upload_dict_to_s3(json.dumps(json_filez1, indent=2, default=myconverter), bucket_name, filename_zeroshot1, aws_access_key_id=storage_access_key, aws_secret_access_key=storage_secret_key)\n",
    "else:\n",
    "    print(\"Invalid destination_storage_type: {destination_storage_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6cd74f0b-9a89-4079-b5d9-58ced793abb6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Added date info code  in Pyspark \n",
    "def replace_only_special_chars(text):\n",
    "    if all(not i.isalnum() for i in text):\n",
    "        return np.nan\n",
    "    else:\n",
    "        return text\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "import hidateinfer\n",
    "\n",
    "def get_date_info(df, date_col):\n",
    "    date_info_dict = {}\n",
    "    \n",
    "    # Clean and filter date values\n",
    "    clean_date_udf = udf(lambda x: replace_only_special_chars(str(x)), StringType())\n",
    "    dates = df.select(date_col).dropna().withColumn(date_col, clean_date_udf(col(date_col)))\n",
    "    \n",
    "    if dates.count() > 0:\n",
    "        # Infer date format\n",
    "        sample_frac = min(1000, rows_count)/rows_count\n",
    "        sampled_dates = dates.sample(False, sample_frac, seed=42)\n",
    "        date_format = hidateinfer.infer(sampled_dates.select(date_col).rdd.flatMap(lambda x: x).collect())\n",
    "        date_info_dict[\"date_format\"] = date_format\n",
    "        \n",
    "        # Get most frequent date\n",
    "        mode = dates.groupBy(date_col).count().orderBy(col(\"count\").desc()).first()\n",
    "        most_freq_date = mode[date_col]\n",
    "        cardinality = mode[\"count\"]\n",
    "        date_info_dict[\"most_freq_date\"] = f\"{most_freq_date} with cardinality = {cardinality}\"\n",
    "    \n",
    "    return date_info_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "72551bab-f032-4b07-90c6-5a8019248ef3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    date_cols = [column for column in sdf.columns if infer_datatypes(sdf, column)['Majority Type'] == 'Date']\n",
    "    date_columns_info = {}\n",
    "    for i in date_cols:\n",
    "        date_info_dict = get_date_info(sdf, i)\n",
    "        date_columns_info[i] = date_info_dict\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    date_columns_info = {\"date_column\" : {\"date_format\" : \"No Date column found\", \"most_freq_date\": \"No Date column found\"}}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8206c6b5-e3bd-4790-a8e4-a72c269c501e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def character_analysis_samples(df):\n",
    "    new = df.copy().fillna(\"\").astype(str)\n",
    "    col_char_samples = defaultdict(dict)\n",
    "    col_char_counts = {}\n",
    "    for column in df.columns:\n",
    "        \n",
    "        new[column + \"_sp_chars\"] = new[column].apply(lambda x : check_special_char(x))\n",
    "        new[column + \"_whitespaces\"] = new[column].apply(lambda x : check_whitespaces(x))\n",
    "        new[column + \"_non_ascii_chars\"] = new[column].apply(lambda x : check_non_ascii(x))\n",
    "        new[column + \"_alphanums\"] = new[column].apply(lambda x : check_alnum(x))\n",
    "\n",
    "        new[column + \"_sp_chars_count\"] = new[column + \"_sp_chars\"].map(len)\n",
    "        new[column + \"_whitespaces_count\"] = new[column + \"_whitespaces\"].map(len)\n",
    "        new[column + \"_non_ascii_chars_count\"] = new[column + \"_non_ascii_chars\"].map(len)\n",
    "        new[column + \"_alphanums_count\"] = new[column + \"_alphanums\"].map(len)\n",
    "        \n",
    "        add_cols = [column + x for x in [\"_sp_chars_count\", \"_whitespaces_count\", \"_non_ascii_chars_count\", \"_alphanums_count\"]]\n",
    "        new.loc[:, column+ \"_all_chars_count\"] = new[add_cols].sum(axis=1)\n",
    "            \n",
    "        for a,b in zip([\"special_chars\", \"whitespaces\", \"non_ascii\", \"alphanum\"], [\"_sp_chars_count\", \"_whitespaces_count\", \"_non_ascii_chars_count\", \"_alphanums_count\"]):\n",
    "            try:\n",
    "                col_char_samples[column][a] = df[new[column + b] !=0].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            except:\n",
    "                col_char_samples[column][a] = df[new[column + b] !=0].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "        \n",
    "        dicty = {}\n",
    "        total = len(new)\n",
    "        \n",
    "        #sp_chars_perc = float(\"{:.2f}\".format((sum(new[column + \"_sp_chars_count\"] !=0)/total)*100)) #+ \"%\" #np.nan_to_num(sp_chars_perc)\n",
    "        #whitespaces_perc = float(\"{:.2f}\".format((sum(new[column + \"_whitespaces_count\"] !=0)/total)*100))\n",
    "        #non_ascii_perc = float(\"{:.2f}\".format((sum(new[column + \"_non_ascii_chars_count\"] !=0)/total)*100))\n",
    "        #alnum_perc = float(\"{:.2f}\".format((sum(new[column + \"_alphanums_count\"] !=0)/total)*100))\n",
    "\n",
    "        dicty[\"special_chars\"] = sum(new[column + \"_sp_chars_count\"] !=0)\n",
    "        dicty[\"whitespaces\"] = sum(new[column + \"_whitespaces_count\"] !=0)\n",
    "        dicty[\"non_ascii\"] = sum(new[column + \"_non_ascii_chars_count\"] !=0)\n",
    "        dicty[\"alphanum\"] = sum(new[column + \"_alphanums_count\"] !=0)\n",
    "        col_char_counts[column] = dicty\n",
    "    \n",
    "    return new, col_char_counts, col_char_samples\n",
    "\n",
    "\n",
    "cols1 = list(df.columns[:int(len(df.columns)/2)])\n",
    "cols2 = list(df.columns[int(len(df.columns)/2):])\n",
    "\n",
    "char_df1, col_char_counts1, col_char_samples1 = character_analysis_samples(df[cols1])\n",
    "char_df2, col_char_counts2, col_char_samples2 = character_analysis_samples(df[cols2])\n",
    "\n",
    "col_char_counts = {**col_char_counts1, **col_char_counts2}\n",
    "col_char_samples = {**col_char_samples1, **col_char_samples2}\n",
    "char_df = pd.concat([char_df1, char_df2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ef96c7c-bc1f-4a85-8d1d-8bdf32fa63ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def get_col_char_dist_samples(ddf, char_df):\n",
    "    df = ddf.fillna(\"\").astype(str)\n",
    "    col_char_dists_samples = {}\n",
    "    col_char_dists = defaultdict(dict)\n",
    "    for column in df.columns:\n",
    " \n",
    "        char_group_samples = defaultdict(dict)\n",
    "        for a,b in zip([\"special_chars\", \"whitespaces\", \"non_ascii\", \"alphanum\"], [\"_sp_chars\", \"_whitespaces\", \"_non_ascii_chars\", \"_alphanums\"]):\n",
    "            chars = [x for xs in char_df[column + b].to_list() for x in xs]\n",
    "            non_others = [k for k, v in Counter(chars).most_common()[:11]]\n",
    "            others = [k for k, v in Counter(chars).most_common()[11:]]\n",
    "            \n",
    "            chars_list = [\"Others\" if ele in others else ele for ele in chars]\n",
    "            col_char_dists[column][a] = dict(Counter(chars_list))\n",
    "            \n",
    "            \n",
    "            for char in non_others:\n",
    "                try:\n",
    "                    char_group_samples[a][char] = ddf[df[column].str.contains(re.escape(char))].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "                except:\n",
    "                    char_group_samples[a][char] = ddf[df[column].str.contains(re.escape(char))].fillna(\"\").to_dict(\"records\")\n",
    "                    \n",
    "            try:\n",
    "                char_group_samples[a][\"Others\"] = ddf[df[column].apply(lambda x: True if any(i in x for i in others) else False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            except:  \n",
    "                char_group_samples[a][\"Others\"] = ddf[df[column].apply(lambda x: True if any(i in x for i in others) else False)].fillna(\"\").to_dict(\"records\")\n",
    "                    \n",
    "        col_char_dists_samples[column] = dict(char_group_samples)\n",
    "            \n",
    "    return dict(col_char_dists), col_char_dists_samples\n",
    "\n",
    "\n",
    "cols1 = list(df.columns[:int(len(df.columns)/2)])\n",
    "cols2 = list(df.columns[int(len(df.columns)/2):])\n",
    "\n",
    "col_char_dists1, col_char_dists_samples1 = get_col_char_dist_samples(df[cols1], char_df)\n",
    "col_char_dists2, col_char_dists_samples2 = get_col_char_dist_samples(df[cols2], char_df)\n",
    "\n",
    "col_char_dists= {**col_char_dists1, **col_char_dists2}\n",
    "col_char_dists_samples = {**col_char_dists_samples1, **col_char_dists_samples2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42eb034-1a87-4ea0-aea5-ce337e0dbf95",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def getExpressionSymbolsFromCharacter(character):\n",
    "\n",
    "    # Constants for SmartDqPattern\n",
    "    smartDqDigit = 'd'\n",
    "    smartDqUpper = 'A'\n",
    "    smartDqLower = 'a'\n",
    "    smartDqWhiteSpace = ' '\n",
    "    \n",
    "    \n",
    "    regExDigit = \"\\\\d+\"\n",
    "    regExUpper = \"[A-Z]+\"\n",
    "    regExLower = \"[a-z]+\"\n",
    "    regExWhiteSpace = \"[\\\\n\\\\t\\\\r]+\"\n",
    "    regExSpecialChar = \".+?\"\n",
    "\n",
    "    if (character.isdigit()):\n",
    "        return smartDqDigit\n",
    "    elif (character.isupper()):\n",
    "        return smartDqUpper\n",
    "    elif (character.islower()):\n",
    "        return smartDqLower\n",
    "    elif (character.isspace()):\n",
    "        return smartDqWhiteSpace\n",
    "    else:\n",
    "        return character\n",
    "      \n",
    "        \n",
    "def PatternString(string):\n",
    "    \n",
    "    if string in [\"nan\", \"NAN\", \"NaN\", \"Nan\", np.nan]:\n",
    "        return \"NULL\"\n",
    "    else:\n",
    "        smartDQPatternBuilder = \"\"\n",
    "        \n",
    "    for character in string:\n",
    "        smartDQPatternBuilder = smartDQPatternBuilder + getExpressionSymbolsFromCharacter(character)\n",
    "\n",
    "    return smartDQPatternBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7cd77a01-31af-4df1-a65f-11ecc22fbd0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.io as pio\n",
    "\n",
    "\n",
    "def get_pattern_histograms_samples(ddf):\n",
    "    \n",
    "    df = ddf.copy().astype(str)\n",
    "    pattern_jsons= {}\n",
    "    pattern_plot_jsons = {}\n",
    "    pattern_samples_jsons = defaultdict(dict)\n",
    "    \n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x : PatternString(x))\n",
    "        pattern_jsons[column] = df[column].value_counts(dropna=False).to_dict()\n",
    "        \n",
    "        others = list(df[~df[column].isin(df[column].value_counts().reset_index(name=\"count\").head(10)[\"index\"].values)][column].unique())\n",
    "        df.loc[df[column].isin(others), column] = \"Others\"\n",
    "        \n",
    "        dfg = df.groupby([column]).size().to_frame().sort_values([0], ascending = False).reset_index()\n",
    "        dfg.columns = [column, \"count\"]\n",
    "        dfg[\"percentage\"] = dfg[\"count\"].div(dfg[\"count\"].sum(), axis=0).multiply(100).apply(lambda x : float(\"{:.2f}\".format(x)))\n",
    "        pattern_plot_jsons[column] = dfg.to_dict(\"records\")\n",
    "\n",
    "        for val in dfg[column].values:\n",
    "            try:\n",
    "                pattern_samples_jsons[column][val] = ddf[df[column]==val].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            except:\n",
    "                pattern_samples_jsons[column][val] = ddf[df[column]==val].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "    return pattern_jsons, pattern_plot_jsons, pattern_samples_jsons\n",
    "    \n",
    "pattern_jsons, pattern_plot_jsons, pattern_samples_jsons = get_pattern_histograms_samples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85201b4a-ccf4-4168-a204-ae288927a37f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def standardize(word):\n",
    "    if \"ad\" in word:\n",
    "        return \"ad\"\n",
    "    elif \"a\" in word:\n",
    "        return \"a\"\n",
    "    elif \"d\" in word:\n",
    "        return \"d\"\n",
    "    else:\n",
    "        return word\n",
    "\n",
    "def get_garbage_values(ddf):\n",
    "    df = ddf.copy().astype(str)\n",
    "    for column in df.columns:\n",
    "        df[column] = df[column].apply(lambda x : PatternString(x))\n",
    "        df[column] = df[column].apply(lambda x : \"\".join(sorted( set( x.lower() ) ) ).replace(\" \", \"\") )\n",
    "        df[column] = df[column].apply(lambda x : standardize(x))\n",
    "\n",
    "    df = df.replace(\"lnu\", \"NULL\")\n",
    "    \n",
    "    pat_dict = {}\n",
    "    for col in df.columns:\n",
    "        pat_dict[col] = df[col].value_counts().to_dict()\n",
    "\n",
    "     \n",
    "    garbage_dict1 ={}\n",
    "    for k, v in pat_dict.items():\n",
    "        garbage_list1 = []\n",
    "        for i in v.copy().keys():\n",
    "            if i not in [\"a\", \"ad\", \"d\"]:\n",
    "                garbage_list1.append(i)\n",
    "                del v[i]\n",
    "        garbage_dict1[k] = garbage_list1\n",
    "\n",
    "\n",
    "    for k, v in pat_dict.items():\n",
    "        s = sum(v.values())\n",
    "        for x,y in v.items():\n",
    "            v[x] = y * 100.0 / s\n",
    "\n",
    "\n",
    "    garbage_dict2 = {}\n",
    "    for k, v in pat_dict.items():\n",
    "        garbage_list2 = []\n",
    "        for x,y in v.items():\n",
    "            if (list(v.keys())[0] ==\"d\") and (y < 2):\n",
    "                garbage_list2.append(x)\n",
    "        garbage_dict2[k] = garbage_list2\n",
    "\n",
    "    for k in garbage_dict2.copy().keys():\n",
    "        garbage_dict2[k] = garbage_dict2[k] + garbage_dict1[k]\n",
    "\n",
    "    \n",
    "    garbage_vals_dict ={}\n",
    "    for col in ddf.columns:\n",
    "        garbage_vals_dict[col] = set(ddf[df[col].isin(garbage_dict2[col])][col].unique())\n",
    "    \n",
    "    return garbage_vals_dict\n",
    "\n",
    "garbage_vals_dict = get_garbage_values(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91b6fc5a-de67-417f-85af-59328ad5836d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def garb_values_dist_samples(ddf, garbage_vals_dict):\n",
    "    #df = ddf.copy()\n",
    "    garbage_vals_dists = defaultdict(dict)\n",
    "    garbage_vals_counts = defaultdict(dict)\n",
    "    garbage_vals_samples = defaultdict(dict)\n",
    "\n",
    "    for column in ddf.columns:\n",
    "        \n",
    "        garb_vals = ddf[ddf[column].isin(garbage_vals_dict[column])][column].to_list()\n",
    "        sorted_counts = pd.Series(garb_vals).value_counts(dropna=False).index.to_list()\n",
    "        non_others = sorted_counts[:11]\n",
    "        others = sorted_counts[11:]\n",
    "        garbage_vals_counts[column][\"garbage_values\"] = len(garb_vals)\n",
    "        \n",
    "        garb_vals_list =[\"Others\" if ele in others else ele for ele in garb_vals]\n",
    "        garbage_vals_dists[column][\"garbage_values\"] = pd.Series(garb_vals_list).value_counts(dropna=False).to_dict()\n",
    "\n",
    "        for val in non_others:\n",
    "            try:\n",
    "                garbage_vals_samples[column][val] = ddf[ddf[column].isin([val])].sample(20).astype(str).to_dict(\"records\")\n",
    "            except:\n",
    "                garbage_vals_samples[column][val] = ddf[ddf[column].isin([val])].astype(str).to_dict(\"records\")\n",
    "        try:\n",
    "            garbage_vals_samples[column][\"Others\"] = ddf[ddf[column].isin(others)].sample(20).astype(str).to_dict(\"records\")\n",
    "        except:\n",
    "            garbage_vals_samples[column][\"Others\"] = ddf[ddf[column].isin(others)].astype(str).to_dict(\"records\")\n",
    "        \n",
    "    return garbage_vals_counts, garbage_vals_dists, garbage_vals_samples\n",
    "\n",
    "\n",
    "global splitl\n",
    "def splitl(a, n):\n",
    "    k, m = divmod(len(a), n)\n",
    "    return (a[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "col_list = list(splitl(list(df.columns), 3))\n",
    "\n",
    "garbage_vals_counts1, garbage_vals_dists1, garbage_vals_samples1 = garb_values_dist_samples(df[col_list[0]], garbage_vals_dict)\n",
    "garbage_vals_counts2, garbage_vals_dists2, garbage_vals_samples2 = garb_values_dist_samples(df[col_list[1]], garbage_vals_dict)\n",
    "garbage_vals_counts3, garbage_vals_dists3, garbage_vals_samples3 = garb_values_dist_samples(df[col_list[2]], garbage_vals_dict)\n",
    "\n",
    "garbage_vals_counts = {**garbage_vals_counts1, **garbage_vals_counts2, **garbage_vals_counts3}\n",
    "garbage_vals_dists = {**garbage_vals_dists1, **garbage_vals_dists2, **garbage_vals_dists3}\n",
    "garbage_vals_samples = {**garbage_vals_samples1, **garbage_vals_samples2, **garbage_vals_samples3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab5726c-2752-4eff-b9a3-ec3b38a13e6f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "key = random.sample(list(pattern_plot_jsons.keys()),1)[0]\n",
    "preview_outputz2 = {\"Pattern_Analysis_Plots\" : pattern_plot_jsons[key]}\n",
    "\n",
    "json_filez2 = {\"Date_Columns_Info\": json.dumps(date_columns_info),\n",
    "             \"Char_Counts\": col_char_counts,\n",
    "             \"Gargbage_Val_Counts\": dict(garbage_vals_counts), \n",
    "             \"Pattern_Analysis\" : [pattern_jsons], #json.dumps(pattern_dict)\n",
    "             \"Pattern_Analysis_Plots\" : [pattern_plot_jsons]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eefdf61f-503a-432f-9d09-04764dce82fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Commented Python code for dtypes\n",
    "# dtypes_samples ={}\n",
    "# for i in inferred_dtypes_.keys():\n",
    "#     try:\n",
    "#         dtypes_samples[i] = df[inferred_dtypes_[i]].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "#     except:\n",
    "#         dtypes_samples[i] = df[inferred_dtypes_[i]].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "# Added Pyspark code for dtypes\n",
    "all_inferred_cols = {column:infer_datatypes(sdf, column)['Majority Type'] for column in sdf.columns }\n",
    "\n",
    "grouped_dtypes = {}\n",
    "for k, v in all_inferred_cols.items():\n",
    "    if v not in grouped_dtypes:\n",
    "        grouped_dtypes[v] = [k]\n",
    "    else:\n",
    "        grouped_dtypes[v].append(k)\n",
    "\n",
    "dtypes_samples = {}\n",
    "for i in grouped_dtypes.keys():\n",
    "    dtypes_samples[i] = sdf[grouped_dtypes[i]].limit(min(20, rows_count)).na.fill(\"\").toPandas().to_dict(\"records\")\n",
    "\n",
    "def null_unique_distinct_analysis_samples(ddf):\n",
    "    df = ddf.copy()\n",
    "    null_notnull_samples = defaultdict(dict)\n",
    "    unique_duplicate_samples = defaultdict(dict)\n",
    "    unique_distinct_samples = defaultdict(dict)\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            null_notnull_samples[column][\"Null\"] = df[df[column].isnull()].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            null_notnull_samples[column][\"Null\"] = df[df[column].isnull()].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "        try:\n",
    "            null_notnull_samples[column][\"Not_Null\"] = df[df[column].notnull()].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            null_notnull_samples[column][\"Not_Null\"] = df[df[column].notnull()].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "\n",
    "        try:\n",
    "            unique_duplicate_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            unique_distinct_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            unique_duplicate_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].fillna(\"\").to_dict(\"records\")\n",
    "            unique_distinct_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "        try:\n",
    "            unique_duplicate_samples[column][\"Duplicate\"] = df[df.duplicated(subset=column, keep=False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            unique_duplicate_samples[column][\"Duplicate\"] = df[df.duplicated(subset=column, keep=False)].fillna(\"\").to_dict(\"records\")\n",
    "            \n",
    "        try:\n",
    "            unique_distinct_samples[column][\"Distinct\"] = df[df[column].isin(set(df[column].unique()))].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            unique_distinct_samples[column][\"Distinct\"] = df[df[column].isin(set(df[column].unique()))].fillna(\"\").to_dict(\"records\")\n",
    "            \n",
    "    return null_notnull_samples, unique_duplicate_samples, unique_distinct_samples\n",
    "\n",
    "def null_unique_distinct_analysis_samples_v2(ddf):\n",
    "    df = ddf.copy()\n",
    "    null_notnull_samples = defaultdict(dict)\n",
    "    unique_duplicate_samples = defaultdict(dict)\n",
    "    unique_distinct_samples = defaultdict(dict)\n",
    "    for column in df.columns:\n",
    "        try:\n",
    "            null_notnull_samples[column][\"Null\"] = df[df[column].isnull()].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            null_notnull_samples[column][\"Null\"] = df[df[column].isnull()].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "        df_not_null = df[df[column].notnull()].fillna(\"\")\n",
    "        d1 = df_not_null[column].value_counts().to_dict()            \n",
    "        d1 = {k:f\"{k} ({v})\" for k,v in d1.items()}\n",
    "        df_not_null[column] = df_not_null[column].map(d1)\n",
    "        null_notnull_samples[column][\"Not_Null\"] = df_not_null.sample(min(20, len(df_not_null))).to_dict(\"records\")\n",
    "\n",
    "        try:\n",
    "            unique_duplicate_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            unique_distinct_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            unique_duplicate_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].fillna(\"\").to_dict(\"records\")\n",
    "            unique_distinct_samples[column][\"Unique\"] = df[~df.duplicated(subset=column, keep=False)].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "        df_dups = df[df.duplicated(subset=column, keep=False)].fillna(\"\")\n",
    "        d1 = df_dups[column].value_counts().to_dict()            \n",
    "        d1 = {k:f\"{k} ({v})\" for k,v in d1.items()}\n",
    "        df_dups[column] = df_dups[column].map(d1)\n",
    "        df_dups = df_dups.groupby(column, group_keys=True).apply(lambda x:x.head(2)).reset_index(drop=True).head(20)         \n",
    "        unique_duplicate_samples[column][\"Duplicate\"] = df_dups.to_dict(\"records\")\n",
    "            \n",
    "        try:\n",
    "            unique_distinct_samples[column][\"Distinct\"] = df[df[column].isin(set(df[column].unique()))].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "        except:\n",
    "            unique_distinct_samples[column][\"Distinct\"] = df[df[column].isin(set(df[column].unique()))].fillna(\"\").to_dict(\"records\")\n",
    "            \n",
    "    return null_notnull_samples, unique_duplicate_samples, unique_distinct_samples\n",
    "\n",
    "def value_counts_dist_samples(ddf):\n",
    "    df = ddf.copy()\n",
    "    value_count_dist_jsons = {}\n",
    "    value_count_sample_jsons = defaultdict(dict)\n",
    "\n",
    "    for column in df.columns:\n",
    "        others = list(df[~df[column].isin(df[column].value_counts().reset_index(name=\"count\").head(10)[\"index\"].values)][column].unique())\n",
    "        df.loc[df[column].isin(others), column] = \"Others\"\n",
    "\n",
    "        dfg = df.groupby([column]).size().to_frame().sort_values([0], ascending = False).reset_index()\n",
    "        dfg.columns = [column, \"count\"]\n",
    "        dfg[\"percentage\"] = dfg[\"count\"].div(dfg[\"count\"].sum(), axis=0).multiply(100).apply(lambda x : float(\"{:.2f}\".format(x)))\n",
    "        value_count_dist_jsons[column] = dfg.to_dict(\"records\")\n",
    "        \n",
    "        for val in dfg[column].values:\n",
    "            try:\n",
    "                value_count_sample_jsons[column][val] = ddf[df[column]==val].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            except:\n",
    "                value_count_sample_jsons[column][val] = ddf[df[column]==val].fillna(\"\").to_dict(\"records\")\n",
    "    return value_count_dist_jsons, value_count_sample_jsons\n",
    "\n",
    "\n",
    "def length_dist_samples(ddf):\n",
    "    df = ddf.copy()\n",
    "    #ddf = df.copy()\n",
    "    length_dist_jsons = {}\n",
    "    length_samples_jsons = defaultdict(dict)\n",
    "    for column in df.columns:\n",
    "        df[column + \"_len_\"] = df[column].fillna(\"\").astype(str).str.len().astype(str)\n",
    "        others = list(df[~df[column + \"_len_\"].isin(df[column + \"_len_\"].value_counts().reset_index(name=\"count\").head(10)[\"index\"].values)][column + \"_len_\"].unique())\n",
    "        df.loc[df[column + \"_len_\"].isin(others), column + \"_len_\"] = \"Others\"\n",
    "\n",
    "        dfg = df.groupby([column + \"_len_\"]).size().to_frame().sort_values([0], ascending = False).reset_index()\n",
    "        dfg.columns = [column + \"_len_\", \"count\"]\n",
    "        dfg[\"percentage\"] = dfg[\"count\"].div(dfg[\"count\"].sum(), axis=0).multiply(100).apply(lambda x : float(\"{:.2f}\".format(x)))\n",
    "        length_dist_jsons[column] = dfg.to_dict(\"records\")\n",
    "\n",
    "        for length in dfg[column + \"_len_\"].values:\n",
    "            #print(column, others)\n",
    "            try:\n",
    "                length_samples_jsons[column][length] = ddf[df[column + \"_len_\"]==length].sample(20).fillna(\"\").to_dict(\"records\")\n",
    "            except:\n",
    "                length_samples_jsons[column][length] = ddf[df[column + \"_len_\"]==length].fillna(\"\").to_dict(\"records\")\n",
    "\n",
    "    return length_dist_jsons, length_samples_jsons\n",
    "\n",
    "\n",
    "# d = json.loads(length_samples_jsons['first_name']['Others'])\n",
    "# pd.DataFrame.from_dict(d)\n",
    "\n",
    "null_notnull_samples_jsons, unique_duplicate_sample_jsons, unique_distinct_sample_jsons = null_unique_distinct_analysis_samples_v2(df)\n",
    "value_count_dist_jsons, value_count_sample_jsons = value_counts_dist_samples(df)\n",
    "length_dist_jsons, length_samples_jsons = length_dist_samples(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2c89e4e2-8ac7-4a48-a209-9e0de36cc787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    rec_sample = df.sample(50).fillna(\"\").to_dict(\"records\")\n",
    "except:\n",
    "    rec_sample = df.fillna(\"\").to_dict(\"records\")\n",
    "    \n",
    "zs_drill_down_outputs = {\"Record_count\": [rec_sample],\n",
    "                     \"Data_types\": [dtypes_samples],\n",
    "                     \"Null_NotNull\": [dict(null_notnull_samples_jsons)],\n",
    "                     \"Unique_Duplicate\": [dict(unique_duplicate_sample_jsons)],\n",
    "                     \"Unique_Distinct\" : [dict(unique_distinct_sample_jsons)],\n",
    "                     \"Most_Frequent\": [value_count_dist_jsons, dict(value_count_sample_jsons)],\n",
    "                     \"Character_Analysis\": [dict(col_char_dists), dict(col_char_samples), col_char_dists_samples],\n",
    "                     \"Garbage_Value_Analysis\": [dict(garbage_vals_dists), dict(garbage_vals_samples)],\n",
    "                     \"Length_Analysis\" : [length_dist_jsons, dict(length_samples_jsons)],\n",
    "                     \"Pattern_Analysis\": [pattern_plot_jsons, dict(pattern_samples_jsons)] }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9daea2f5-3dc3-42ff-a79f-9961bb0cd622",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#from numpyencoder import NumpyEncoder\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "class NpEncoder(json.JSONEncoder):\n",
    "    \"\"\" Custom encoder for numpy data types \"\"\"\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, (np.int_, np.intc, np.intp, np.int8,\n",
    "                            np.int16, np.int32, np.int64, np.uint8,\n",
    "                            np.uint16, np.uint32, np.uint64, np.integer)):\n",
    "\n",
    "            return int(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.float_, np.float16, np.float32, np.float64, np.floating)):\n",
    "            return float(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.complex_, np.complex64, np.complex128)):\n",
    "            return {'real': obj.real, 'imag': obj.imag}\n",
    "\n",
    "        elif isinstance(obj, (np.ndarray)):\n",
    "            return obj.tolist()\n",
    "\n",
    "        elif isinstance(obj, (np.bool_)):\n",
    "            return bool(obj)\n",
    "\n",
    "        elif isinstance(obj, (np.void)): \n",
    "            return None\n",
    "        \n",
    "        elif isinstance(obj, datetime.datetime):\n",
    "            return obj.__str__()\n",
    "        \n",
    "        elif isinstance(obj, (np.generic, np.ndarray)):\n",
    "            return obj.item()\n",
    "\n",
    "        return json.JSONEncoder.default(self, obj)\n",
    "    \n",
    "    \n",
    "class CustomJSONizer(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        return super().encode(int(obj)) \\\n",
    "            if isinstance(obj, np.int64) \\\n",
    "            else super().default(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eaea9dd0-d3cb-43aa-b8e0-02d3488e9a2f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Store outputs in differnet storage based on destination_storage_type\n",
    "\n",
    "if destination_storage_type == 'ABS':\n",
    "\n",
    "    blob_clientz2 = BLOB_SERVICE_CLIENT.get_blob_client(\n",
    "        container=CONTAINER_NAME, blob=filename_zeroshot2)\n",
    "    blob_clientz2.upload_blob(json.dumps(json_filez2, indent=2).encode('utf-8'), overwrite=True)\n",
    "\n",
    "    blob_clientzp2 = BLOB_SERVICE_CLIENT.get_blob_client(\n",
    "        container=CONTAINER_NAME, blob=f'{filename_zeroshot2}.preview')\n",
    "    blob_clientzp2.upload_blob(json.dumps(preview_outputz2, indent=2).encode('utf-8'), overwrite=True)\n",
    "\n",
    "    blob_clientzdd = BLOB_SERVICE_CLIENT.get_blob_client(\n",
    "        container=CONTAINER_NAME, blob=f'{filename_zeroshot1}.zdd')\n",
    "    blob_clientzdd.upload_blob(json.dumps(zs_drill_down_outputs, indent=2, default=myconverter, skipkeys=True).encode('utf-8'), overwrite=True) #encoding='UTF-8' inside dumps\n",
    "\n",
    "elif destination_storage_type == 'S3':\n",
    "\n",
    "    upload_dict_to_s3(json.dumps(json_filez2, indent=2, default=myconverter), bucket_name, filename_zeroshot2, aws_access_key_id=storage_access_key, aws_secret_access_key=storage_secret_key)\n",
    "\n",
    "    upload_dict_to_s3(json.dumps(preview_outputz2, indent=2, default=myconverter), bucket_name, f'{filename_zeroshot2}.preview', aws_access_key_id=storage_access_key, aws_secret_access_key=storage_secret_key)\n",
    "\n",
    "    upload_dict_to_s3(json.dumps(zs_drill_down_outputs, indent=2, default=myconverter), bucket_name, f'{filename_zeroshot1}.zdd', aws_access_key_id=storage_access_key, aws_secret_access_key=storage_secret_key)\n",
    "else:\n",
    "    print(\"Invalid destination_storage_type: {destination_storage_type}\")\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Zeroshot_conc1",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
